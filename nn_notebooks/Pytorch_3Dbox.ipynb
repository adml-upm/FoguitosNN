{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vx Variations NN without hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Hyperparameter tuning imports\n",
    "# from ray import tune\n",
    "# from ray import train\n",
    "# from ray.train import Checkpoint, get_checkpoint\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "# import ray.cloudpickle as cloudpickle\n",
    "\n",
    "results_dir = r\"C:\\Users\\Admin\\Documents\\Alexander\\ANSYS learning\\test_results\\Unified_vx_var\\results\"\n",
    "data_file = os.path.join(results_dir, 'res_vec.npz')\n",
    "loaded_data = np.load(data_file)['arr_0']\n",
    "label_file = os.path.join(results_dir, 'res_idx.npy')\n",
    "labels_array = np.load(label_file)\n",
    "\n",
    "# Get parameters for normalization:\n",
    "data_means = torch.from_numpy(np.array([np.mean(loaded_data[:, i]) for i in range(0, loaded_data.shape[1])]))\n",
    "data_vars = torch.from_numpy(np.array([np.var(loaded_data[:, i]) if np.var(loaded_data[:, i]) != 0 else 1 for i in range(0, loaded_data.shape[1])]))\n",
    "x_data = torch.from_numpy(loaded_data)\n",
    "\n",
    "print('Data tensor shape: {}'.format(x_data.shape))\n",
    "print(f\"Datatype of tensor: {x_data.dtype}\")\n",
    "if torch.cuda.is_available():\n",
    "  x_data = x_data.to('cuda')\n",
    "print(f\"Device tensor is stored on: {x_data.device}\")\n",
    "\n",
    "training_set_prop = 0.8\n",
    "test_set_prop = 1 - training_set_prop\n",
    "# index splitting loaded data valid only if data saved in random order. We randomize just in case\n",
    "final_training_set_index = int(training_set_prop * len(x_data))\n",
    "training_data = x_data[0:final_training_set_index, :]\n",
    "test_data = x_data[final_training_set_index:, :]\n",
    "\n",
    "training_data.requires_grad = True\n",
    "test_data.requires_grad = True\n",
    "\n",
    "\n",
    "class CustomConvectionDataset(Dataset):\n",
    "    def __init__(self, label_file, data_tensor, data_means, data_vars, transform=None, target_transform=None):\n",
    "        self.labels = np.load(label_file)\n",
    "        self.output_data_positions = np.char.startswith(self.labels, 'h')\n",
    "        self.input_data_positions = ~ self.output_data_positions\n",
    "        self.data = data_tensor\n",
    "        self.data_means = data_means\n",
    "        self.data_vars = data_vars\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            data_row = self.transform(self.data[idx, :], self.data_means, self.data_vars)\n",
    "        else:\n",
    "            data_row = self.data[idx, :]\n",
    "        input_data = data_row[self.input_data_positions]\n",
    "        results_data = data_row[self.output_data_positions]\n",
    "        return input_data.float(), results_data.float()\n",
    "\n",
    "def input_transf(dataset_row, data_means, data_vars):\n",
    "    # Given a row of data from the dataset, perform: (x - mean) / var\n",
    "    return torch.div(torch.sub(dataset_row, data_means), data_vars)\n",
    "\n",
    "def output_transf(dataset_row, data_means, data_vars):\n",
    "\t# Given a row of data from the dataset, perform: y * var + mean -> reverse of input_transf\n",
    "\treturn torch.sub(torch.mul(dataset_row, data_vars), data_means)\n",
    "\n",
    "def nn_output_transf(nn_output, data_means, data_vars):\n",
    "\t# Given the output of the NN, perform: y * var + mean -> reverse of input_transf on output data\n",
    "\tmask = np.char.startswith(labels_array, 'h')  # Boolean mask for labels that start with an 'h'\n",
    "\tfiltered_data_means = data_means[torch.tensor(mask)]\n",
    "\tfiltered_data_vars = data_vars[torch.tensor(mask)]\n",
    "\treturn torch.sub(torch.mul(nn_output, filtered_data_vars), filtered_data_means)\n",
    "    \n",
    "TrainingDataset = CustomConvectionDataset(label_file=label_file, data_tensor=training_data, data_means=data_means, data_vars=data_vars, transform=input_transf)\n",
    "TestingDataset = CustomConvectionDataset(label_file=label_file, data_tensor=test_data, data_means=data_means, data_vars=data_vars, transform=input_transf)\n",
    "\n",
    "print(len(TrainingDataset))\n",
    "print(TrainingDataset.__getitem__(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Dataloader: Good for shuffling between epochs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(TrainingDataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(TestingDataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Display data for a given param\n",
    "train_data_row = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, network_arch):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(network_arch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fwd_pass_steps_in_order = self.linear_relu_stack(x)\n",
    "        return fwd_pass_steps_in_order\n",
    "\n",
    "num_n_input_layer = 12\n",
    "num_n_intermediate_layer = 20\n",
    "num_n_output_layer = 13\n",
    "network_arch = OrderedDict([\n",
    "\t('lin1', nn.Linear(num_n_input_layer, num_n_intermediate_layer)),\n",
    "\t('relu1', nn.Tanh()),\n",
    "\t('lin2', nn.Linear(num_n_intermediate_layer, num_n_output_layer)),\n",
    "\t# ('relu2', nn.ReLU()),\n",
    "\t])\n",
    "\n",
    "model = NeuralNetwork(network_arch=network_arch).to(device)\n",
    "print(model)\n",
    "print(model.parameters())\n",
    "print(sum([len(m) for m in list(model.parameters())]))\n",
    "\n",
    "print('testing:')\n",
    "input = torch.randn(num_n_input_layer)\n",
    "print(input.shape)\n",
    "print(model(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a model, we need a loss function and an optimizer.\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, printer=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        # Note that X is the NN input vector, y is the correct result for the given input X\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()  # computes gradient of loss function\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch_idx+1 % 5 == 0 and printer:\n",
    "            loss, current = loss.item(), (batch_idx + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return loss\n",
    "\n",
    "# Check the models performance to see that it is learning\n",
    "def test(dataloader, model, loss_fn, printer=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            accuracy = torch.nn.functional.mse_loss(pred, y, reduce='mean')  # element-wise mean squared error\n",
    "    test_loss /= num_batches\n",
    "    accuracy /= size\n",
    "    if printer:\n",
    "        print(f\"Test Error: \\n L2 loss: {(accuracy):>0.1f}, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing through n Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "\t'test_loss': [],\n",
    "\t'accuracy': [],\n",
    "}\n",
    "\n",
    "epochs = 120\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss, test_accuracy = test(test_dataloader, model, loss_fn)\n",
    "    # Save training progress to historical sequence\n",
    "    history['train_loss'].append(float(train_loss))\n",
    "    history['test_loss'].append(float(test_loss))\n",
    "    history['accuracy'].append(float(test_accuracy))\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# Create the figure and subplots\n",
    "n_subplots = 3\n",
    "epoch_axis_data = list(range(1, epochs+1))\n",
    "fig, axs = plt.subplots(n_subplots, 1, sharex=True, figsize=(8, 6))\n",
    "for idx, (label, history) in zip(range(n_subplots), history.items()):\n",
    "    axs[idx].plot(epoch_axis_data, history)\n",
    "    axs[idx].set_ylabel(label)\n",
    "\n",
    "fig.suptitle('Training history: Progression through epochs')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(results_dir, \"model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models and making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(network_arch=network_arch).to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "x, y = TrainingDataset.__getitem__(40)\n",
    "with torch.no_grad():\n",
    "\tx = x.to(device)\n",
    "\tpred = model(x)\n",
    "\tprint(f'Input data: {x}')\n",
    "\tprint(f'Predicted: {pred}')\n",
    "\tprint(f'Actual: {y}')\n",
    "\tprint(f\"Total error: {torch.nn.functional.mse_loss(pred, y, reduce='mean')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
